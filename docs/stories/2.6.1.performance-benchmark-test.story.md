# Story 2.6.1: Performance Benchmark Test

## Status

**Draft**

---

## Story

**As a** developer,
**I want** automated performance benchmarks for document processing,
**so that** I can validate NFR1 compliance and prevent performance regressions.

---

## Acceptance Criteria

1. **AC1:** Automated pytest-benchmark test validates NFR1: Process 2-5K word documents in <30 seconds on standard hardware (4-core CPU, 8GB RAM).
2. **AC2:** Benchmark test runs against realistic test documents with varied entity densities (low: <10 entities, medium: 10-30, high: 30+ entities).
3. **AC3:** Performance baseline documented: Mean processing time, standard deviation, min/max values.
4. **AC4:** CI/CD integration: Benchmark runs on PR builds with performance regression detection (>20% slowdown fails build).
5. **AC5:** Performance metrics logged: Processing time breakdown (NLP detection, database ops, file I/O, replacements).

---

## Parent Story

**Deferred from:** Story 2.6 - Single-Document Pseudonymization Workflow
**QA Gate Reference:** TEST-001 (Low severity, deferred per MVP scope)
**Original AC:** Story 2.6 Task 6 (Performance Validation)

---

## Tasks / Subtasks

- [ ] **Task 1: Create Performance Test Documents** (AC: 1, 2)
  - [ ] Generate 2000-word test document (low entity density: ~8 entities)
  - [ ] Generate 3500-word test document (medium entity density: ~20 entities)
  - [ ] Generate 5000-word test document (high entity density: ~35 entities)
  - [ ] Store in `tests/fixtures/performance/` directory
  - [ ] Document entity counts and expected processing characteristics

- [ ] **Task 2: Implement Pytest-Benchmark Tests** (AC: 1, 3)
  - [ ] Create `tests/performance/test_document_processing_benchmark.py`
  - [ ] Implement `test_single_document_2k_words` benchmark
  - [ ] Implement `test_single_document_3500_words` benchmark
  - [ ] Implement `test_single_document_5k_words` benchmark
  - [ ] Configure pytest-benchmark settings (warmup rounds, iterations, timeout)
  - [ ] Assert NFR1 threshold: `benchmark.stats['mean'] < 30.0`

- [ ] **Task 3: Add Performance Profiling** (AC: 5)
  - [ ] Instrument DocumentProcessor with timing breakpoints
  - [ ] Log time spent in: entity detection, pseudonym assignment, database saves, file I/O
  - [ ] Output performance breakdown to benchmark results
  - [ ] Identify bottlenecks (if any) for future optimization

- [ ] **Task 4: CI/CD Integration** (AC: 4)
  - [ ] Add benchmark step to GitHub Actions workflow (if using GitHub)
  - [ ] Configure performance regression detection (compare against baseline)
  - [ ] Store benchmark history for trend analysis
  - [ ] Document performance baseline in `docs/performance-baseline.md`

- [ ] **Task 5: Documentation** (AC: 3, 5)
  - [ ] Document baseline performance metrics
  - [ ] Create performance troubleshooting guide
  - [ ] Document how to run benchmarks locally: `poetry run pytest tests/performance/ --benchmark-only`

---

## Dev Notes

### Parent Story Context

**From Story 2.6 (Single-Document Pseudonymization Workflow):**
- DocumentProcessor already implemented with complete workflow in [gdpr_pseudonymizer/core/document_processor.py](gdpr_pseudonymizer/core/document_processor.py)
- Manual testing shows ~12-13s for 3000-word documents
- Batch saves optimization already applied (Story 2.6 QA fixes) - expect 50-80% improvement over individual saves
- In-memory cache for duplicate entities within document

### NFR Requirements

**NFR1 (Performance - Single Document):**
- **Target:** Process 2-5K word document in <30 seconds on standard hardware
- **Standard Hardware:** 4-core CPU, 8GB RAM (define explicitly in test documentation)
- **Current Status:** Manual validation only; no automated regression tests

### Performance Bottlenecks to Profile

Based on Story 2.6 architecture review:
1. **NLP Entity Detection:** spaCy model inference (likely largest time consumer)
2. **Database Operations:** Entity lookups + batch save
3. **File I/O:** Read input, write output
4. **String Replacements:** Apply pseudonyms to text

### Testing Standards

**Test File Location:**
```
tests/performance/
└── test_document_processing_benchmark.py

tests/fixtures/performance/
├── sample_2000_words.txt
├── sample_3500_words.txt
└── sample_5000_words.txt
```

**Pytest-Benchmark Configuration:**
- Warmup rounds: 3
- Test iterations: 5-10 (balance accuracy vs CI time)
- Timeout: 60 seconds per benchmark
- Store results in `.benchmarks/` directory

**Test Pattern:**
```python
import pytest
from gdpr_pseudonymizer.core.document_processor import DocumentProcessor

@pytest.mark.benchmark(group="document-processing")
def test_single_document_3500_words(benchmark, test_db, tmp_path):
    """NFR1: Process 3500-word document in <30 seconds."""
    # Arrange
    input_file = "tests/fixtures/performance/sample_3500_words.txt"
    output_file = tmp_path / "output.txt"
    processor = DocumentProcessor(
        db_path=test_db,
        passphrase="test_passphrase_12345",
        theme="neutral"
    )

    # Act & Assert
    result = benchmark(processor.process_document, input_file, str(output_file))

    # Validate NFR1 threshold
    assert benchmark.stats['mean'] < 30.0, f"Processing took {benchmark.stats['mean']:.2f}s (threshold: 30s)"
    assert result.success is True
```

**Commands:**
```bash
# Run benchmarks
poetry run pytest tests/performance/ --benchmark-only

# Compare against baseline
poetry run pytest tests/performance/ --benchmark-compare=0001

# Save baseline
poetry run pytest tests/performance/ --benchmark-save=baseline
```

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2026-01-28 | 1.0 | Story created from Story 2.6 deferred item (TEST-001). Automated performance benchmarks for NFR1 validation. | Quinn (Test Architect) |

---

## Dev Agent Record

*To be populated by Dev Agent during implementation*

---

## QA Results

*To be populated by QA Agent after Dev Agent completes implementation*
