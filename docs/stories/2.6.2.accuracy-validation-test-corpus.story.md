# Story 2.6.2: Accuracy Validation Against Test Corpus

## Status

**Superseded by Story 4.4**

---

## Story

**As a** quality engineer,
**I want** automated accuracy validation against a ground-truth test corpus,
**so that** I can verify NER accuracy meets NFR8/NFR9 thresholds (<10% false negatives, <15% false positives).

---

## Acceptance Criteria

1. **AC1:** Test corpus with ground-truth annotations: 25+ documents with manually verified entity labels (PERSON, LOCATION, ORG).
2. **AC2:** Automated test calculates false negative rate: `(missed_entities / total_ground_truth_entities) * 100 < 10%` (NFR8).
3. **AC3:** Automated test calculates false positive rate: `(incorrect_detections / total_detections) * 100 < 15%` (NFR9).
4. **AC4:** Per-entity-type accuracy metrics: Separate FN/FP rates for PERSON, LOCATION, ORG entities.
5. **AC5:** Accuracy report generated: Summary statistics, per-document breakdown, failure cases documented.
6. **AC6:** CI/CD integration: Accuracy test runs on NLP model changes with regression detection.

---

## Parent Story

**Deferred from:** Story 2.6 - Single-Document Pseudonymization Workflow
**QA Gate Reference:** TEST-002 (Low severity, deferred per MVP scope)
**Original AC:** Story 2.6 Task 7 (Accuracy Validation)
**Dependency:** Story 1.1 test corpus (if available)

---

## Tasks / Subtasks

- [ ] **Task 1: Create or Locate Test Corpus** (AC: 1)
  - [ ] Check if Story 1.1 test corpus exists (25 documents with ground truth)
  - [ ] If exists: Verify ground-truth annotations are machine-readable (JSON/YAML format)
  - [ ] If missing: Create test corpus with 25 French documents
  - [ ] Manual annotation: Mark all PERSON, LOCATION, ORG entities with character offsets
  - [ ] Store in `tests/fixtures/accuracy_corpus/` directory
  - [ ] Document annotation schema and quality control process

- [ ] **Task 2: Implement Ground-Truth Parser** (AC: 1)
  - [ ] Create `tests/utils/corpus_parser.py`
  - [ ] Parse ground-truth annotations into `GroundTruthEntity` objects
  - [ ] Handle annotation format (JSON/YAML/inline markup)
  - [ ] Validate annotation completeness (no missing required fields)

- [ ] **Task 3: Implement Accuracy Calculation** (AC: 2, 3, 4)
  - [ ] Create `tests/accuracy/test_ner_accuracy_validation.py`
  - [ ] Implement entity matching logic (text + type must match for TP)
  - [ ] Calculate metrics:
    - True Positives (TP): Detected entities matching ground truth
    - False Positives (FP): Detected entities NOT in ground truth
    - False Negatives (FN): Ground-truth entities NOT detected
    - False Negative Rate: `FN / (TP + FN) * 100`
    - False Positive Rate: `FP / (TP + FP) * 100`
  - [ ] Implement per-entity-type breakdown (PERSON, LOCATION, ORG)

- [ ] **Task 4: Implement Accuracy Test** (AC: 2, 3, 6)
  - [ ] Load all corpus documents
  - [ ] Process each document through DocumentProcessor (same as production)
  - [ ] Compare detected entities against ground truth
  - [ ] Assert NFR8: `false_negative_rate < 10.0`
  - [ ] Assert NFR9: `false_positive_rate < 15.0`
  - [ ] Log failures with document references for debugging

- [ ] **Task 5: Generate Accuracy Report** (AC: 5)
  - [ ] Create `AccuracyReport` class
  - [ ] Output summary statistics (overall FN/FP rates, entity counts)
  - [ ] Output per-document breakdown (which docs have highest error rates)
  - [ ] Output failure case analysis (which entity types/patterns cause errors)
  - [ ] Generate markdown report in `tests/reports/accuracy_report.md`
  - [ ] Include recommendations for NLP model tuning (if needed)

- [ ] **Task 6: CI/CD Integration** (AC: 6)
  - [ ] Add accuracy test to GitHub Actions workflow
  - [ ] Run on: NLP model changes, major refactoring, weekly schedule
  - [ ] Store accuracy history for trend analysis
  - [ ] Alert on regression (FN/FP rates exceed thresholds)

- [ ] **Task 7: Documentation** (AC: 5)
  - [ ] Document test corpus creation process
  - [ ] Document annotation schema
  - [ ] Document accuracy baseline metrics
  - [ ] Create troubleshooting guide for accuracy failures

---

## Dev Notes

### Parent Story Context

**From Story 2.6 (Single-Document Pseudonymization Workflow):**
- HybridEntityDetector already implemented in [gdpr_pseudonymizer/nlp/hybrid_detector.py](gdpr_pseudonymizer/nlp/hybrid_detector.py)
- Combines spaCy NER + regex patterns for improved accuracy
- DocumentProcessor uses detector in production workflow

**From Story 1.1 (NLP Entity Detection):**
- Original NLP validation may have created test corpus (check `tests/fixtures/` or similar)
- If Story 1.1 test corpus exists, reuse it for this validation

### NFR Requirements

**NFR8 (Accuracy - False Negatives):**
- **Target:** <10% false negative rate (missed entities)
- **Calculation:** `(entities_in_ground_truth_but_not_detected / total_ground_truth_entities) * 100`
- **Mitigation:** Hybrid detection (spaCy + regex) already improves recall

**NFR9 (Accuracy - False Positives):**
- **Target:** <15% false positive rate (incorrect detections)
- **Calculation:** `(entities_detected_but_not_in_ground_truth / total_detected_entities) * 100`
- **Mitigation:** Validation mode (Epic 3) will allow user corrections for high-stakes documents

### Test Corpus Requirements

**Corpus Size:** 25+ documents (per Story 2.6 Task 7)
**Document Characteristics:**
- Variety of entity densities (low: <10 entities, medium: 10-30, high: 30+ entities)
- Real-world French text (letters, reports, emails)
- Mix of entity types: PERSON (primary), LOCATION, ORG
- Edge cases: Titles (Dr., M., Mme.), compound names (Jean-Pierre), ambiguous entities

**Annotation Schema (Suggested JSON Format):**
```json
{
  "document_id": "doc_001",
  "text": "Marie Dubois travaille à Paris.",
  "ground_truth_entities": [
    {
      "text": "Marie Dubois",
      "entity_type": "PERSON",
      "start_pos": 0,
      "end_pos": 12
    },
    {
      "text": "Paris",
      "entity_type": "LOCATION",
      "start_pos": 26,
      "end_pos": 31
    }
  ]
}
```

### Entity Matching Logic

**Match Criteria (for True Positive):**
1. **Text Match:** Detected entity text == Ground-truth entity text (case-sensitive)
2. **Type Match:** Detected entity type == Ground-truth entity type
3. **Position Match (optional):** Start/end positions match (use for strict validation)

**Fuzzy Matching Considerations:**
- Character offsets may differ slightly (e.g., whitespace differences)
- Consider implementing token-based matching instead of strict character offsets
- Document matching strategy in code comments

### Testing Standards

**Test File Location:**
```
tests/accuracy/
└── test_ner_accuracy_validation.py

tests/fixtures/accuracy_corpus/
├── documents/
│   ├── doc_001.txt
│   ├── doc_002.txt
│   └── ...
└── annotations/
    ├── doc_001.json
    ├── doc_002.json
    └── ...

tests/utils/
└── corpus_parser.py

tests/reports/
└── accuracy_report.md  (generated by test)
```

**Test Pattern:**
```python
import pytest
from gdpr_pseudonymizer.nlp.hybrid_detector import HybridDetector
from tests.utils.corpus_parser import load_ground_truth_corpus

@pytest.mark.accuracy
def test_ner_accuracy_against_corpus():
    """NFR8/NFR9: Validate accuracy against ground-truth test corpus."""
    # Arrange
    corpus = load_ground_truth_corpus("tests/fixtures/accuracy_corpus/")
    detector = HybridDetector()

    total_tp = 0
    total_fp = 0
    total_fn = 0

    # Act
    for doc in corpus:
        detected = detector.detect_entities(doc.text)
        gt_entities = doc.ground_truth_entities

        # Calculate TP, FP, FN per document
        tp, fp, fn = calculate_metrics(detected, gt_entities)
        total_tp += tp
        total_fp += fp
        total_fn += fn

    # Calculate rates
    false_negative_rate = (total_fn / (total_tp + total_fn)) * 100
    false_positive_rate = (total_fp / (total_tp + total_fp)) * 100

    # Assert NFR thresholds
    assert false_negative_rate < 10.0, f"FN rate: {false_negative_rate:.2f}% (threshold: <10%)"
    assert false_positive_rate < 15.0, f"FP rate: {false_positive_rate:.2f}% (threshold: <15%)"
```

**Commands:**
```bash
# Run accuracy validation
poetry run pytest tests/accuracy/ -v

# Run with detailed report
poetry run pytest tests/accuracy/ -v --tb=short

# Generate accuracy report
poetry run pytest tests/accuracy/ --accuracy-report=tests/reports/accuracy_report.md
```

### Known Challenges

**Challenge 1: Ground-Truth Quality**
- Manual annotation is time-consuming and error-prone
- Consider double-annotation + adjudication for high-quality corpus
- Document annotation guidelines clearly

**Challenge 2: Entity Matching Ambiguity**
- "Marie" vs "Marie Dubois" - should partial matches count?
- Solution: Match on full entity text only (strict matching)

**Challenge 3: NLP Model Variability**
- spaCy model version changes may affect accuracy
- Lock spaCy model version in dependencies
- Re-run accuracy validation when upgrading NLP models

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2026-01-28 | 1.0 | Story created from Story 2.6 deferred item (TEST-002). Automated accuracy validation against ground-truth test corpus for NFR8/NFR9. | Quinn (Test Architect) |
| 2026-02-08 | 1.1 | Status changed to "Superseded by Story 4.4". Story 4.4 encompasses full scope of 2.6.2 plus confidence analysis, regression comparison, monitoring baselines review, and quality report. | Dev Agent (Story 4.4) |

---

## Dev Agent Record

*To be populated by Dev Agent during implementation*

---

## QA Results

*To be populated by QA Agent after Dev Agent completes implementation*
