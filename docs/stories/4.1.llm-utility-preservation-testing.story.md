# Story 4.1: LLM Utility Preservation Testing

## Status

**Done**

---

## Story

**As a** product manager,
**I want** rigorous validation that pseudonymized documents maintain usefulness for LLM analysis,
**so that** I can confidently market the primary value proposition to LLM users.

---

## Context

This story validates NFR10 (LLM Utility Preservation) - the core value proposition of the GDPR Pseudonymizer. The tool is specifically designed for users who need to process sensitive documents with LLMs while maintaining GDPR compliance. If pseudonymization degrades LLM analysis quality, the tool loses its primary value.

**Why This Matters:**
- Users choose this tool specifically to enable LLM analysis of sensitive documents
- Pseudonymization must preserve semantic relationships and document coherence
- Quality threshold: ≥80% utility preservation (NFR10)

**Prerequisites:**
- Epic 3 complete (all entity types - PERSON, LOCATION, ORG - pseudonymized)
- Test corpus available (25 documents in `tests/test_corpus/`)
- LLM API access required (OpenAI and/or Anthropic)

---

## Acceptance Criteria

### AC1: LLM API Access Configured
- OpenAI API (ChatGPT) and/or Anthropic API (Claude) keys obtained
- Testing budget allocated ($50-100 estimated)
- API access validated with simple test calls

### AC2: Test Set Prepared
- 10 representative documents selected from existing test corpus:
  - 5 interview transcripts (from `tests/test_corpus/interview_transcripts/`)
  - 5 business documents (from `tests/test_corpus/business_documents/`)
- Documents selected per Technical Assumptions protocol (representative of target use cases)
- Both original and pseudonymized versions prepared for each document

### AC3: Standardized Prompts Executed
- Three standardized prompts executed for both original and pseudonymized versions:
  1. "Summarize the main themes in this document"
  2. "Identify key relationships between individuals mentioned"
  3. "Extract action items and decisions made"
- Results captured for all 10 documents x 2 versions x 3 prompts = 60 LLM responses

### AC4: Evaluation Rubric Applied
- Evaluation rubric applied to each prompt pair (original vs pseudonymized):
  - Thematic accuracy (1-5 scale)
  - Relationship coherence (1-5 scale)
  - Factual preservation (1-5 scale)
  - Overall utility (1-5 scale)
- Rubric scores documented for all comparisons

### AC5: Results Analyzed
- Average utility score calculated across all evaluations
- Target: ≥4.0/5.0 (80% threshold from NFR10)
- Statistical summary: mean, median, standard deviation, min/max

### AC6: Edge Cases Documented
- Scenarios where pseudonymization degraded utility identified and documented
- Recommendations for users on how to mitigate utility loss
- Known limitations clearly stated

### AC7: Results Documented
- Methodology document created: `docs/llm-validation-report.md`
- Findings with example comparisons (original vs pseudonymized LLM outputs)
- Clear pass/fail determination against NFR10 threshold

### AC8: Go/No-Go Decision
- If ≥80% utility: Document as PASS, proceed with launch
- If <80% utility: Determine if acceptable with caveats OR requires changes before launch
- Decision documented with rationale

---

## Tasks / Subtasks

- [x] **Task 4.1.1: Set Up LLM API Access** (AC: 1)
  - [x] Obtain API keys (OpenAI GPT-4 and/or Anthropic Claude)
  - [x] Store keys securely (environment variables, not in code)
  - [x] Create simple test script to validate API connectivity
  - [x] Document API setup in dev notes (for reproducibility)

- [x] **Task 4.1.2: Select and Prepare Test Documents** (AC: 2)
  - [x] Review available test corpus documents
  - [x] Select 5 representative interview transcripts
  - [x] Select 5 representative business documents
  - [x] Document selection criteria (entity density, document length, entity type mix)
  - [x] Create list of selected documents with paths

- [x] **Task 4.1.3: Generate Pseudonymized Versions** (AC: 2)
  - [x] Process all 10 selected documents through pseudonymization pipeline
  - [x] Use neutral theme for consistency
  - [x] Verify all entity types (PERSON, LOCATION, ORG) pseudonymized correctly
  - [x] Store pseudonymized versions alongside originals

- [x] **Task 4.1.4: Create LLM Testing Script** (AC: 3)
  - [x] Create script: `scripts/llm_utility_test.py`
  - [x] Implement API calls to OpenAI and/or Anthropic
  - [x] Implement the 3 standardized prompts
  - [x] Implement response capture and logging
  - [x] Add rate limiting to avoid API throttling
  - [x] Handle API errors gracefully

- [x] **Task 4.1.5: Execute LLM Tests** (AC: 3)
  - [x] Run all prompts on all 10 original documents
  - [x] Run all prompts on all 10 pseudonymized documents
  - [x] Capture all 60 responses
  - [x] Store responses in structured format (JSON or similar)

- [x] **Task 4.1.6: Create Evaluation Rubric** (AC: 4)
  - [x] Define scoring criteria for each dimension (1-5 scale):
    - Thematic accuracy: How well does the response capture main themes?
    - Relationship coherence: Are entity relationships correctly identified?
    - Factual preservation: Are facts/details accurately extracted?
    - Overall utility: Would this response be useful to a researcher?
  - [x] Document rubric with examples of each score level
  - [x] Create evaluation spreadsheet or form

- [x] **Task 4.1.7: Evaluate LLM Responses via LLM-as-Judge** (AC: 4)
  - [x] Implement LLM-as-judge scoring in test script (see Dev Notes for prompt)
  - [x] Run automated evaluation on all 30 comparison pairs (10 docs x 3 prompts)
  - [x] Store scores in structured format (JSON)
  - [x] Flag any pairs with significant quality differences (score delta ≥2)

- [x] **Task 4.1.8: Analyze Results** (AC: 5)
  - [x] Calculate average utility score across all evaluations
  - [x] Calculate statistics: mean, median, std dev, min/max
  - [x] Determine if ≥80% threshold met (average ≥4.0/5.0)
  - [x] Break down results by entity type, document type, prompt type

- [x] **Task 4.1.9: Document Edge Cases** (AC: 6)
  - [x] Identify cases where utility dropped significantly
  - [x] Analyze root causes (entity type, context loss, etc.)
  - [x] Write recommendations for users
  - [x] Document known limitations

- [x] **Task 4.1.10: Create Validation Report** (AC: 7)
  - [x] Create `docs/llm-validation-report.md`
  - [x] Document methodology (test selection, prompts, rubric)
  - [x] Include example comparisons with LLM outputs
  - [x] Present statistical findings
  - [x] Include appendix with full results

- [x] **Task 4.1.11: Make Go/No-Go Recommendation** (AC: 8)
  - [x] Determine pass/fail against NFR10 threshold
  - [x] Document decision with rationale
  - [x] If fail: propose mitigation options
  - [x] Get PM sign-off on recommendation

---

## Dev Notes

### Story Type
This is a **validation/testing story**, not a pure code implementation story. The primary output is:
1. Test execution and data collection
2. Analysis and evaluation
3. Documentation (validation report)

### LLM API Usage
**[Source: Epic 4 PRD]**

- **Budget:** $50-100 allocated for API testing
- **APIs:** OpenAI (GPT-4) and/or Anthropic (Claude)
- **Security:** API keys must be stored in environment variables, never committed to code
- **Rate Limiting:** Implement delays between API calls to avoid throttling

**Recommended Environment Variables:**
```bash
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
```

### Test Corpus Available
**[Source: Codebase - tests/test_corpus/]**

**Interview Transcripts (15 available, select 5):**
- `tests/test_corpus/interview_transcripts/interview_01.txt` through `interview_15.txt`
- Recommended: interview_01, interview_03, interview_05, interview_07, interview_10
- Selection criteria: Variety of entity types, different lengths, different topics

**Business Documents (10 available, select 5):**
- `tests/test_corpus/business_documents/`:
  - `audit_summary.txt`, `board_minutes.txt`, `contract_memo.txt`
  - `email_chain.txt`, `hr_announcement.txt`, `incident_report.txt`
  - `meeting_minutes.txt`, `partnership_agreement.txt`, `project_report.txt`, `sales_proposal.txt`
- Recommended: meeting_minutes, email_chain, incident_report, hr_announcement, contract_memo
- Selection criteria: Variety of document types, entity density, business context

### Pseudonymization Commands
**[Source: architecture/19-coding-standards.md]**

```bash
# Create output directory
mkdir -p tests/test_corpus/llm_test

# Initialize database
poetry run gdpr-pseudo init --db tests/test_corpus/llm_test/test_llm.db

# Process single document (example)
poetry run gdpr-pseudo process tests/test_corpus/interview_transcripts/interview_01.txt \
  --db tests/test_corpus/llm_test/test_llm.db \
  --theme neutral \
  --output tests/test_corpus/llm_test/interview_01_pseudo.txt

# Use validation mode to ensure quality
poetry run gdpr-pseudo process <input_file> \
  --db tests/test_corpus/llm_test/test_llm.db \
  --validate
```

### Standardized Prompts
**[Source: Epic 4 PRD - AC3]**

The three prompts to use for all documents:

1. **Theme Summary:**
   ```
   Summarize the main themes in this document. Provide a concise overview of the key topics discussed.
   ```

2. **Relationship Identification:**
   ```
   Identify key relationships between individuals mentioned in this document. Describe who interacts with whom and in what context.
   ```

3. **Action Items:**
   ```
   Extract action items and decisions made in this document. List specific tasks, deadlines, or resolutions if mentioned.
   ```

### Evaluation Rubric Guidelines
**[Source: Epic 4 PRD - AC4]**

**Scoring Scale (1-5):**
- 5 = Excellent - No degradation, identical or equivalent quality
- 4 = Good - Minor differences, still highly useful
- 3 = Acceptable - Some information loss, but core meaning preserved
- 2 = Poor - Significant degradation, limited utility
- 1 = Unacceptable - Major errors or missing information

**Thematic Accuracy:**
- Does the response capture the same main themes?
- Are topic priorities correctly identified?

**Relationship Coherence:**
- Are entity relationships (who works with whom, reporting structures) preserved?
- Do pseudonymized names maintain consistent references?

**Factual Preservation:**
- Are specific facts, dates, locations correctly extracted?
- Are action items and decisions accurately identified?

**Overall Utility:**
- Would a researcher find this response equally useful?
- Does pseudonymization introduce confusion or errors?

### Evaluation Method: LLM-as-Judge
**[Source: Story validation - PO decision]**

Task 4.1.7 uses **automated LLM-as-judge** evaluation. The test script should include a scoring function that:

1. Sends each response pair to Claude (or GPT-4) with a structured scoring prompt
2. Extracts numeric scores for each dimension
3. Stores results programmatically

**LLM-as-Judge Prompt Template:**
```
You are evaluating the quality of LLM responses to pseudonymized documents compared to original documents.

ORIGINAL DOCUMENT RESPONSE:
{original_response}

PSEUDONYMIZED DOCUMENT RESPONSE:
{pseudonymized_response}

PROMPT USED: "{prompt_text}"

Score the pseudonymized response compared to the original on these dimensions (1-5 scale):
- Thematic Accuracy: Does it capture the same main themes? (5=identical, 1=completely different)
- Relationship Coherence: Are entity relationships correctly preserved? (5=perfect, 1=broken)
- Factual Preservation: Are facts/details accurately extracted? (5=all preserved, 1=major errors)
- Overall Utility: Would a researcher find this equally useful? (5=equivalent, 1=useless)

Respond in JSON format:
{"thematic_accuracy": X, "relationship_coherence": X, "factual_preservation": X, "overall_utility": X, "notes": "brief explanation if any score < 4"}
```

**Rate Limiting:** 1 request/second for scoring calls (separate from document analysis calls)

### NFR10 Threshold
**[Source: PRD NFRs]**

- **Target:** ≥80% utility preservation
- **Measurement:** Average score ≥4.0/5.0 across all evaluations
- **Pass Criteria:** If average ≥4.0, NFR10 is met
- **Fail Handling:** If <4.0, determine if acceptable with documented caveats or if changes needed

### File Locations for Outputs
**[Source: architecture/12-unified-project-structure.md]**

- **Test script:** `scripts/llm_utility_test.py`
- **Test database:** `tests/test_corpus/llm_test/test_llm.db`
- **Pseudonymized files:** `tests/test_corpus/llm_test/` (create this directory)
  - Naming: `{original_name}_pseudo.txt` (e.g., `interview_01_pseudo.txt`)
- **LLM responses:** `docs/qa/llm-test-responses.json`
- **Evaluation scores:** `docs/qa/llm-evaluation-scores.json`
- **Validation report:** `docs/llm-validation-report.md`

### Previous Story Insights
**[Source: Story 3.9 Dev Agent Record]**

- All entity types (PERSON, LOCATION, ORG) now properly detected
- Professional titles (Maître, Me) preserved in output
- Cabinet patterns correctly detected as ORG
- These improvements should positively impact LLM utility (relationship coherence)

---

## Testing

### Testing Standards
**[Source: architecture/16-testing-strategy.md]**

This story is primarily a **manual testing and evaluation** story, not automated test code.

**Deliverables:**
1. Test script (`scripts/llm_utility_test.py`) - for executing LLM API calls
2. Evaluation data (spreadsheet or JSON with scores)
3. Validation report (`docs/llm-validation-report.md`)

**No new pytest tests required** - this is a validation study, not code implementation.

**Quality Assurance:**
- All LLM responses saved for reproducibility
- Evaluation methodology documented
- Results independently verifiable

### Test Execution Commands
```bash
# Process test documents (see Dev Notes for full command)
poetry run gdpr-pseudo process tests/test_corpus/interview_transcripts/interview_01.txt \
  --db tests/test_corpus/llm_test/test_llm.db \
  --theme neutral \
  --output tests/test_corpus/llm_test/interview_01_pseudo.txt

# Run LLM utility test script (once created)
poetry run python scripts/llm_utility_test.py

# Script outputs:
# - docs/qa/llm-test-responses.json (all 60 LLM responses)
# - docs/qa/llm-evaluation-scores.json (LLM-as-judge scores)
```

---

## Change Log

| Date       | Version | Description                          | Author         |
|------------|---------|--------------------------------------|----------------|
| 2026-02-06 | 1.0     | Initial story draft created from Epic 4 | Bob (SM Agent) |
| 2026-02-06 | 1.1     | Added LLM-as-judge methodology, output paths, db paths | Sarah (PO Agent) |
| 2026-02-06 | 2.0     | Implementation complete - NFR10 PASSED (4.27/5.0) | James (Dev Agent) |

---

## Dev Agent Record

### Agent Model Used

Claude Opus 4.5 (claude-opus-4-5-20251101)

### Debug Log References

No debug log entries required - validation story with no code bugs encountered.

### Completion Notes

- **NFR10 PASSED**: Overall utility score 4.27/5.0 (85.4%), exceeding 80% threshold
- Used Anthropic Claude (claude-sonnet-4-20250514) for testing and LLM-as-judge evaluation
- 90 API calls total: 60 document analysis + 30 evaluations
- Edge cases documented: 6 evaluations scored below 4.0, root causes identified (Org-XXX codes, complex entity mapping)
- Go recommendation: APPROVED for launch with documented caveats

### File List

**New Files:**
- `scripts/llm_utility_test.py` - LLM utility testing script with LLM-as-judge evaluation
- `scripts/validate_api.py` - API connectivity validation script
- `docs/llm-validation-report.md` - Full validation report with methodology and results
- `docs/qa/llm-test-responses.json` - All 60 LLM responses (original + pseudonymized)
- `docs/qa/llm-evaluation-scores.json` - All 30 evaluation scores with statistics
- `tests/test_corpus/llm_test/originals/` - 10 original test documents
- `tests/test_corpus/llm_test/pseudonymized/` - 10 pseudonymized test documents
- `tests/test_corpus/llm_test/test_llm.db` - Test database for pseudonymization
- `.env` - Environment variables file (gitignored, contains API key)

**Modified Files:**
- `pyproject.toml` - Added anthropic and python-dotenv dev dependencies

---

## QA Results

### Review Date: 2026-02-06

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Overall: EXCELLENT** - This validation story demonstrates rigorous methodology and thorough execution. The implementation follows a well-structured A/B testing approach with proper statistical analysis. The code is clean, well-documented, and adheres to project standards.

**Key Strengths:**
- Comprehensive test script with proper dataclass models for type safety
- Rate limiting and error handling for API resilience
- LLM-as-judge evaluation methodology matches industry best practices
- Statistics calculation includes mean, median, std dev for proper analysis
- Results are reproducible with all artifacts saved

### Refactoring Performed

None required - code quality is high and appropriate for a validation script.

### Compliance Check

- Coding Standards: ✓ Type hints, absolute imports, PascalCase/snake_case conventions followed
- Project Structure: ✓ Scripts in `scripts/`, docs in `docs/`, test data in `tests/test_corpus/`
- Testing Strategy: ✓ N/A for validation study (no pytest tests required per story definition)
- All ACs Met: ✓ All 8 acceptance criteria verified with documented evidence

### Requirements Traceability

| AC | Validation Evidence | Status |
|----|---------------------|--------|
| AC1: API Access | `validate_api.py`, `.env` with ANTHROPIC_API_KEY | ✓ PASS |
| AC2: Test Set | 10 docs in `llm_test/originals/` + `pseudonymized/` | ✓ PASS |
| AC3: Prompts Executed | 60 responses in `llm-test-responses.json` | ✓ PASS |
| AC4: Rubric Applied | 30 evaluations in `llm-evaluation-scores.json` | ✓ PASS |
| AC5: Results Analyzed | Statistics: 4.27/5.0 mean, 5.0 median, 0.93 std | ✓ PASS |
| AC6: Edge Cases | 6 low-scoring cases documented in report Section 5 | ✓ PASS |
| AC7: Report | `docs/llm-validation-report.md` with full methodology | ✓ PASS |
| AC8: Go/No-Go | PASS decision with caveats documented | ✓ PASS |

### Improvements Checklist

- [x] All acceptance criteria implemented and verified
- [x] API key security properly handled (environment variables)
- [x] Error handling in LLM calls with graceful degradation
- [x] Rate limiting implemented (1.5s delay between calls)
- [x] JSON parsing with fallback for malformed responses
- [x] Comprehensive documentation in validation report
- [ ] Consider adding retry logic for transient API failures (future enhancement)
- [ ] Consider caching LLM responses for re-evaluation runs (future enhancement)

### Security Review

**Status: PASS**
- API keys stored in `.env` file (gitignored) - not in source code
- `validate_api.py` masks key in output display (`{key[:12]}...{key[-4:]}`)
- No sensitive document content exposed in logs
- Dependencies (`anthropic`, `python-dotenv`) are standard, reputable packages

### Performance Considerations

**Status: ACCEPTABLE**
- 90 total API calls completed successfully
- Rate limiting prevents throttling (1.5s between calls)
- Total tokens used tracked for cost monitoring
- For future: consider async API calls for faster execution

### NFR Validation Summary

| NFR | Status | Evidence |
|-----|--------|----------|
| Security | PASS | API keys in env vars, no secrets in code |
| Performance | PASS | Rate limiting, efficient token usage |
| Reliability | PASS | Error handling, graceful degradation |
| Maintainability | PASS | Clean code, comprehensive docs |
| **NFR10 (LLM Utility)** | **PASS** | **4.27/5.0 (85.4%) > 80% threshold** |

### Files Modified During Review

None - no refactoring required.

### Gate Status

**Gate: PASS** → `docs/qa/gates/4.1-llm-utility-preservation-testing.yml`

**Quality Score: 95/100**
- All ACs met
- NFR10 validated
- No blocking issues
- Minor future enhancements noted

### Recommended Status

✓ **Ready for Done** - All acceptance criteria verified, NFR10 passed with 85.4% utility preservation. Edge cases documented with mitigation strategies. Go recommendation approved for launch.

---

## Story DoD Checklist

### 1. Requirements Met
- [x] All functional requirements specified in the story are implemented
- [x] All acceptance criteria defined in the story are met
  - AC1: API access configured (Anthropic Claude)
  - AC2: Test set prepared (10 documents - 5 interviews, 5 business)
  - AC3: Standardized prompts executed (60 responses captured)
  - AC4: Evaluation rubric applied (30 evaluations with LLM-as-judge)
  - AC5: Results analyzed (4.27/5.0 overall, statistics calculated)
  - AC6: Edge cases documented (6 low-scoring cases identified)
  - AC7: Validation report created (docs/llm-validation-report.md)
  - AC8: Go/No-Go decision documented (PASS - GO for launch)

### 2. Coding Standards & Project Structure
- [x] Code adheres to Operational Guidelines (absolute imports, type hints)
- [x] Code aligns with Project Structure (scripts/ directory)
- [x] Adherence to Tech Stack (Python 3.9+, Poetry)
- [N/A] API/Data Model changes (no core API changes)
- [x] Security best practices (API keys in .env, gitignored)
- [x] No new linter errors (`ruff check` passes)
- [x] Code well-commented (docstrings, inline comments for complex logic)

### 3. Testing
- [N/A] Unit tests (this is a validation study, not code implementation)
- [N/A] Integration tests (not applicable for validation story)
- [x] Existing tests still pass (verified with 96 passing tests)
- [N/A] Test coverage (no new code requiring unit tests)

### 4. Functionality & Verification
- [x] Functionality manually verified (ran full LLM utility test)
- [x] Edge cases handled (documented in report Section 5)

### 5. Story Administration
- [x] All tasks marked complete (11/11 tasks)
- [x] Decisions documented (LLM provider choice, test document selection)
- [x] Story wrap-up complete (Dev Agent Record filled)

### 6. Dependencies, Build & Configuration
- [x] Project builds successfully
- [x] Linting passes
- [x] New dependencies approved: `anthropic`, `python-dotenv` (dev dependencies)
- [x] Dependencies recorded in pyproject.toml
- [x] No security vulnerabilities (standard Anthropic SDK)
- [x] Environment variables documented (.env file, ANTHROPIC_API_KEY)

### 7. Documentation
- [x] Inline documentation (docstrings in llm_utility_test.py)
- [x] User-facing documentation (llm-validation-report.md)
- [N/A] Technical architecture docs (no architectural changes)

### Final Confirmation
- [x] Developer Agent confirms all applicable items addressed

**Summary:** This validation story confirmed NFR10 (LLM Utility Preservation) with a PASS result (4.27/5.0 = 85.4%). All acceptance criteria met, documentation complete, ready for review.
