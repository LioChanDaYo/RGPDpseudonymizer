# Story 2.5: Audit Logging

## Status

**Done**

---

## Story

**As a** compliance officer,
**I want** comprehensive audit logs of all pseudonymization operations,
**so that** I can demonstrate GDPR Article 30 compliance and troubleshoot issues.

---

## Acceptance Criteria

1. **AC1:** Audit log implemented in `operations` table (from Story 2.4 schema).
2. **AC2:** Log entry created for each pseudonymization operation with required fields (FR12): timestamp, operation type, files processed, model version, theme selected, entity count.
3. **AC3:** User modifications logged when validation mode used (deferred to Epic 3, placeholder in schema).
4. **AC4:** Export functionality: Query audit log and export to JSON or CSV format.
5. **AC5:** Log rotation not required for MVP (SQLite database handles growth).
6. **AC6:** Unit tests: log entry creation, query operations, export functionality.
7. **AC7:** Documentation: audit log schema, query examples, GDPR Article 30 mapping.

---

## Tasks / Subtasks

- [x] **Task 1: Export Functionality - JSON Format** (AC: 4, 6)
  - [x] Add `export_to_json(output_path: str, filters)` method to AuditRepository
  - [x] Use `find_operations()` with filters to query operations
  - [x] Serialize Operation objects to JSON with proper datetime formatting
  - [x] Handle file I/O errors gracefully with clear error messages
  - [x] Include metadata in export (schema version, export timestamp, query filters used)
  - [x] Unit tests for JSON export with various filter combinations

- [x] **Task 2: Export Functionality - CSV Format** (AC: 4, 6)
  - [x] Add `export_to_csv(output_path: str, filters)` method to AuditRepository
  - [x] Use Python `csv` module (stdlib) for CSV generation
  - [x] Flatten JSON fields (files_processed, user_modifications) into CSV-compatible strings
  - [x] Include CSV header row with all operation fields
  - [x] Handle file I/O errors gracefully with clear error messages
  - [x] Unit tests for CSV export with edge cases (empty results, special characters)

- [x] **Task 3: Comprehensive Unit Tests for AuditRepository** (AC: 6)
  - [x] Create `tests/unit/test_audit_repository.py`
  - [x] Test `log_operation()`: creates entry with all required fields (FR12)
  - [x] Test `log_operation()`: auto-generates ID and timestamp if not provided
  - [x] Test `find_operations()`: with no filters returns all operations
  - [x] Test `find_operations()`: filter by operation_type (PROCESS, BATCH, etc.)
  - [x] Test `find_operations()`: filter by success=True/False
  - [x] Test `find_operations()`: filter by date range (start_date, end_date)
  - [x] Test `find_operations()`: filter by limit (returns correct count)
  - [x] Test `find_operations()`: ordering (newest first - descending timestamp)
  - [x] Test `get_operation_by_id()`: returns correct operation
  - [x] Test `get_operation_by_id()`: returns None for non-existent ID
  - [x] Test `get_total_entity_count()`: correct sum across operations
  - [x] Test `get_total_entity_count()`: with operation_type filter
  - [x] Test `get_total_entity_count()`: with success filter
  - [x] Test `get_average_processing_time()`: correct average calculation
  - [x] Test `get_average_processing_time()`: with operation_type filter
  - [x] Test `get_average_processing_time()`: returns 0.0 for no operations
  - [x] Test `get_failure_rate()`: correct percentage calculation
  - [x] Test `get_failure_rate()`: with operation_type filter
  - [x] Test `get_failure_rate()`: returns 0.0 for no operations
  - [x] Test `export_to_json()`: creates valid JSON file
  - [x] Test `export_to_json()`: with filters applied
  - [x] Test `export_to_json()`: handles empty results
  - [x] Test `export_to_json()`: datetime serialization (ISO 8601 format)
  - [x] Test `export_to_csv()`: creates valid CSV file
  - [x] Test `export_to_csv()`: with filters applied
  - [x] Test `export_to_csv()`: handles empty results
  - [x] Test `export_to_csv()`: CSV header row present
  - [x] Test `export_to_csv()`: JSON fields flattened correctly
  - [x] Test file I/O errors for both export methods (permission errors, invalid paths)

- [x] **Task 4: Documentation - Audit Log Usage Guide** (AC: 7)
  - [x] Create `docs/architecture/audit-logging.md` (or update existing section)
  - [x] Document operations table schema with all fields explained
  - [x] Provide query examples for common use cases:
    - Find all failed operations in last 7 days
    - Get operations by type (PROCESS vs BATCH)
    - Calculate average processing time per operation type
    - Get failure rate for troubleshooting
  - [x] Document export functionality usage (JSON and CSV formats)
  - [x] Explain when operations are logged (integration points in workflow)
  - [x] Include Python code examples for AuditRepository usage

- [x] **Task 5: Documentation - GDPR Article 30 Mapping** (AC: 7)
  - [x] Create section in `docs/architecture/audit-logging.md` titled "GDPR Article 30 Compliance"
  - [x] Map each Article 30 requirement to operations table fields:
    - "Name and contact details of controller" → Stored externally (user's responsibility)
    - "Purposes of processing" → operation_type field (PROCESS, BATCH, etc.)
    - "Description of data subjects and data categories" → entity_count field
    - "Categories of recipients" → Not applicable (local-only tool, NFR11)
    - "Transfers to third countries" → Not applicable (zero network, NFR11)
    - "Time limits for erasure" → Not enforced (user controls via destroy-table command, FR17)
    - "Technical and organizational security measures" → References to encryption (Story 2.4)
  - [x] Provide guidance on how compliance officers can export audit logs for GDPR audits
  - [x] Include sample audit log export report format
  - [x] Explain limitations: audit logs track operations, not individual entity-level processing (entity table provides that)

- [x] **Task 6: Integration Validation** (AC: 2)
  - [x] Verify `log_operation()` is ready for integration in Story 2.6 workflows
  - [x] Document where Story 2.6 should call `AuditRepository.log_operation()`:
    - After single-document pseudonymization completes (success or failure)
    - Include all FR12 required fields in Operation object
    - Capture processing_time_seconds from workflow timer
    - Capture error_message if operation failed
  - [x] No code changes needed (Story 2.6 will integrate), but document integration points

---

## Dev Notes

### Previous Story Context

**From Story 2.4 (Encrypted Mapping Table):**
- `AuditRepository` class already implemented in [gdpr_pseudonymizer/data/repositories/audit_repository.py](gdpr_pseudonymizer/data/repositories/audit_repository.py) (221 lines)
- Methods already available (Story 2.4 Task 5):
  - `log_operation(operation: Operation) -> Operation` - Creates audit log entry
  - `find_operations(operation_type, success, start_date, end_date, limit) -> list[Operation]` - Query with filters
  - `get_operation_by_id(operation_id: str) -> Optional[Operation]` - Retrieve specific operation
  - `get_total_entity_count(operation_type, success) -> int` - Aggregate statistics
  - `get_average_processing_time(operation_type) -> float` - Performance analytics
  - `get_failure_rate(operation_type) -> float` - Reliability metrics
- Operations table schema already created with all required fields [Source: docs/architecture/9-database-schema.md#68-86]:
  - `id` (TEXT PRIMARY KEY, UUID)
  - `timestamp` (TIMESTAMP, auto-generated)
  - `operation_type` (TEXT: CHECK constraint for PROCESS, BATCH, VALIDATE, IMPORT, EXPORT, DESTROY)
  - `files_processed` (TEXT: JSON array)
  - `model_name` (TEXT: spacy/stanza)
  - `model_version` (TEXT: e.g., fr_core_news_lg-3.7.0)
  - `theme_selected` (TEXT: neutral/star_wars/lotr)
  - `user_modifications` (TEXT: JSON object, optional - deferred to Epic 3)
  - `entity_count` (INTEGER)
  - `processing_time_seconds` (REAL)
  - `success` (BOOLEAN)
  - `error_message` (TEXT, optional)
- Indexes already created for performance [Source: docs/architecture/9-database-schema.md#85-86]:
  - `idx_operations_timestamp` - Fast date range queries
  - `idx_operations_type` - Fast operation type filtering
- **No encryption needed** for operations table (contains metadata only, not sensitive entity data) [Source: gdpr_pseudonymizer/data/repositories/audit_repository.py#6]

**Critical Insight:** Story 2.5 scope is primarily adding export functionality and comprehensive tests. The core AuditRepository implementation is COMPLETE from Story 2.4.

---

### Functional Requirements

**FR12 Definition** [Source: docs/prd/requirements.md#27]:
> The system shall log all pseudonymization operations including timestamp, files processed, entities detected, NLP model name and version, pseudonym theme selected, detection confidence scores (if available), entities modified by user (if validation mode used), and pseudonyms applied.

**FR12 Fields Mapping to Operations Table:**
- timestamp → `timestamp` field ✅
- files processed → `files_processed` field (JSON array) ✅
- entities detected → `entity_count` field ✅
- NLP model name → `model_name` field ✅
- NLP model version → `model_version` field ✅
- pseudonym theme selected → `theme_selected` field ✅
- detection confidence scores → Stored in `entities` table, not operations (design decision: operations log summary, entities table stores details) ✅
- entities modified by user → `user_modifications` field (JSON object, deferred to Epic 3) ⏸️
- pseudonyms applied → Tracked via `entity_count` + entities table relationship ✅

**Note:** FR12 requirement is FULLY MET by current operations table schema. Story 2.5 adds export/testing/documentation only.

---

### GDPR Article 30 Compliance

**Article 30 (Records of Processing Activities) Requirements** [Source: docs/prd/technical-assumptions.md#206]:
- Audit logs (operations table) provide processing activity records
- Operations table tracks: when processing occurred, what files were processed, how many entities detected, which model used, success/failure status
- Export functionality (AC4) enables compliance officers to generate audit reports for GDPR audits
- Transparency requirement: audit logs demonstrate "appropriate technical measures" per Article 32

**GDPR Rights Supported by Audit Logs:**
- **Right to Access (Article 15):** Audit logs show when data subject's data was processed
- **Right to Erasure (Article 17):** `destroy-table` command (FR17) logged in operations table
- **Data Breach Notification (Article 33):** Error logs track failed operations, enabling breach detection
- **Accountability (Article 5.2):** Comprehensive audit trail demonstrates compliance

**Limitations:**
- Audit logs do NOT contain sensitive entity data (operations table has metadata only)
- Individual entity processing history stored in `entities` table (first_seen_timestamp)
- Full GDPR Article 30 compliance requires user to maintain controller/processor documentation externally

---

### Export Functionality Design

**JSON Export Format** (Task 1):
```python
{
  "export_metadata": {
    "schema_version": "1.0.0",
    "export_timestamp": "2026-01-28T14:30:00Z",
    "filters_applied": {
      "operation_type": "PROCESS",
      "success": true,
      "start_date": "2026-01-20T00:00:00Z",
      "end_date": "2026-01-28T23:59:59Z",
      "limit": 100
    },
    "total_results": 15
  },
  "operations": [
    {
      "id": "abc-123-def-456",
      "timestamp": "2026-01-28T14:25:30Z",
      "operation_type": "PROCESS",
      "files_processed": ["input.txt"],
      "model_name": "spacy",
      "model_version": "fr_core_news_lg-3.8.0",
      "theme_selected": "star_wars",
      "user_modifications": null,
      "entity_count": 10,
      "processing_time_seconds": 5.2,
      "success": true,
      "error_message": null
    }
    // ... more operations
  ]
}
```

**CSV Export Format** (Task 2):
- Flatten JSON fields: `files_processed` → comma-separated string `"file1.txt,file2.txt"`
- Flatten JSON fields: `user_modifications` → JSON string `"{"entity_123": "corrected_name"}"`
- CSV headers: id, timestamp, operation_type, files_processed, model_name, model_version, theme_selected, user_modifications, entity_count, processing_time_seconds, success, error_message
- Use Python `csv` module (stdlib) to handle CSV escaping/quoting

**DateTime Serialization:**
- Use ISO 8601 format for JSON: `"2026-01-28T14:30:00Z"` (UTC)
- Use ISO 8601 format for CSV: same format for consistency
- Python: `datetime.isoformat()` method

---

### File Locations

**Source:** [docs/architecture/12-unified-project-structure.md](docs/architecture/12-unified-project-structure.md)

**Existing Files (No Changes Needed):**
```
gdpr_pseudonymizer/data/repositories/
└── audit_repository.py       # Implemented in Story 2.4 - ADD export methods only
```

**New Files to Create:**
```
tests/unit/
└── test_audit_repository.py  # NEW - Comprehensive unit tests (Task 3)

docs/architecture/
└── audit-logging.md           # NEW - Usage guide + GDPR Article 30 mapping (Task 4, 5)
   OR update existing section in 9-database-schema.md if preferred
```

**No Files to Modify** (except audit_repository.py for export methods)

---

### Data Models

**Source:** [docs/architecture/4-data-models.md#59-103](docs/architecture/4-data-models.md)

**Operation Model (SQLAlchemy):**
Already fully defined in [gdpr_pseudonymizer/data/models.py](gdpr_pseudonymizer/data/models.py) lines 75-117.

**Key Fields for Story 2.5:**
- `id`: str (UUID, primary key)
- `timestamp`: datetime (auto-generated if not provided)
- `operation_type`: str (enum with CHECK constraint)
- `files_processed`: str (JSON-serialized list[str])
- `user_modifications`: str (JSON-serialized dict, optional)
- `entity_count`: int
- `processing_time_seconds`: float
- `success`: bool
- `error_message`: str (optional)

**No model changes needed** - Story 2.4 completed all model definitions.

---

### Testing Standards

**Source:** [docs/architecture/16-testing-strategy.md](docs/architecture/16-testing-strategy.md)

**Coverage Target:** Epic 2 target is ≥80% [Source: 16.3]

**Testing Pyramid:** Unit tests (75%) > Integration tests (20%) > E2E (5%) [Source: 16.1]

**Unit Test Requirements (Task 3):**
- Test file: `tests/unit/test_audit_repository.py`
- Use pytest 7.4+ framework [Source: docs/architecture/3-tech-stack.md#18]
- Use pytest fixtures for database setup/teardown (tmp_path, in-memory SQLite)
- Test isolation: each test independent, no shared state [Source: 16.2]
- Descriptive test names: `test_log_operation_creates_entry_with_all_fields()`
- Every test must have docstring explaining what it validates
- Coverage target: ≥90% for AuditRepository (business logic)

**Test Patterns:**

```python
def test_find_operations_filter_by_operation_type(test_db_session):
    """Test filtering operations by operation_type returns correct results."""
    repo = AuditRepository(test_db_session)

    # Create test operations
    repo.log_operation(Operation(operation_type="PROCESS", entity_count=5, processing_time_seconds=2.5, success=True))
    repo.log_operation(Operation(operation_type="BATCH", entity_count=50, processing_time_seconds=15.0, success=True))
    repo.log_operation(Operation(operation_type="PROCESS", entity_count=3, processing_time_seconds=1.8, success=False))

    # Query with filter
    process_ops = repo.find_operations(operation_type="PROCESS")

    # Verify
    assert len(process_ops) == 2
    assert all(op.operation_type == "PROCESS" for op in process_ops)
```

**Export Testing Pattern:**

```python
def test_export_to_json_creates_valid_json_file(test_db_session, tmp_path):
    """Test JSON export creates valid file with correct format."""
    repo = AuditRepository(test_db_session)

    # Create test operation
    repo.log_operation(Operation(
        operation_type="PROCESS",
        files_processed=json.dumps(["test.txt"]),
        model_name="spacy",
        model_version="3.8.0",
        theme_selected="neutral",
        entity_count=10,
        processing_time_seconds=5.2,
        success=True
    ))

    # Export to JSON
    output_path = tmp_path / "audit_export.json"
    repo.export_to_json(str(output_path))

    # Verify file exists and contains valid JSON
    assert output_path.exists()
    with open(output_path) as f:
        data = json.load(f)

    assert "export_metadata" in data
    assert "operations" in data
    assert len(data["operations"]) == 1
    assert data["operations"][0]["operation_type"] == "PROCESS"
```

**CSV Export Testing Pattern:**

```python
def test_export_to_csv_creates_valid_csv_file(test_db_session, tmp_path):
    """Test CSV export creates valid file with header row."""
    repo = AuditRepository(test_db_session)

    # Create test operation
    repo.log_operation(Operation(
        operation_type="BATCH",
        files_processed=json.dumps(["doc1.txt", "doc2.txt"]),
        model_name="spacy",
        model_version="3.8.0",
        theme_selected="star_wars",
        entity_count=25,
        processing_time_seconds=12.5,
        success=True
    ))

    # Export to CSV
    output_path = tmp_path / "audit_export.csv"
    repo.export_to_csv(str(output_path))

    # Verify file exists and contains valid CSV
    assert output_path.exists()
    with open(output_path, newline='') as f:
        reader = csv.DictReader(f)
        rows = list(reader)

    assert len(rows) == 1
    assert rows[0]["operation_type"] == "BATCH"
    assert "doc1.txt,doc2.txt" in rows[0]["files_processed"]
```

**Testing Frameworks Used:**
- pytest 7.4+ (main framework)
- pytest-cov 4.1+ (code coverage measurement)
- Python stdlib modules: `json`, `csv`, `datetime` (for export validation)

---

### Coding Standards

**Source:** [docs/architecture/19-coding-standards.md](docs/architecture/19-coding-standards.md)

**Build Commands (CRITICAL):**
- **ALWAYS use `poetry run` for all commands** [Source: 19.0]
- Tests: `poetry run pytest tests/unit/test_audit_repository.py -v`
- Linting: `poetry run ruff check gdpr_pseudonymizer/`
- Type checking: `poetry run mypy gdpr_pseudonymizer/`

**Module Imports:**
- ALWAYS use absolute imports [Source: 19.1]:
  ```python
  # GOOD
  from gdpr_pseudonymizer.data.repositories.audit_repository import AuditRepository

  # BAD
  from ..repositories.audit_repository import AuditRepository
  ```

**Type Hints:**
- All public functions MUST have complete type hints [Source: 19.1]
- Example:
  ```python
  def export_to_json(
      self,
      output_path: str,
      operation_type: Optional[str] = None,
      success: Optional[bool] = None,
      start_date: Optional[datetime] = None,
      end_date: Optional[datetime] = None,
      limit: Optional[int] = None
  ) -> None:
      """Export operations to JSON file with optional filters."""
  ```

**Logging (CRITICAL):**
- NEVER log sensitive data [Source: 19.1]
- Operations table does NOT contain sensitive data (metadata only), so logging operation_type, entity_count, etc. is SAFE
- GOOD: `logger.info("audit_export_started", format="json", operation_count=len(operations))`
- BAD: `logger.info(f"Exporting: {entity.full_name}")` (would be bad if logging entity data)

**Naming Conventions [Source: 19.2]:**
- Modules: `snake_case` (test_audit_repository.py)
- Classes: `PascalCase` (AuditRepository)
- Functions: `snake_case` (export_to_json, find_operations)
- Constants: `UPPER_SNAKE_CASE` (if any constants needed)

---

### NFR Validation

**NFR Relevance to Story 2.5:**

| NFR | Relevance | Implementation |
|-----|-----------|----------------|
| **NFR11** (Zero network) | ✅ High | Audit logs stored locally in SQLite, export to local files only (no network) |
| **NFR6** (<10% crash rate) | ✅ Medium | Audit logs enable crash rate monitoring via `get_failure_rate()` method |
| **NFR7** (Clear error messages) | ✅ Medium | Export methods must provide clear error messages for file I/O failures |

**No performance NFRs directly apply** (audit logging is background operation, not in critical path for NFR1/NFR2)

---

## Testing

### Test File Location

**Source:** [docs/architecture/12-unified-project-structure.md](docs/architecture/12-unified-project-structure.md)

- **Unit Tests:** `tests/unit/test_audit_repository.py` (Task 3)
- **No integration tests required** (AuditRepository is pure database layer, integration tested in Story 2.6 workflow)

---

### Testing Frameworks

**Source:** [docs/architecture/3-tech-stack.md](docs/architecture/3-tech-stack.md)

- **pytest 7.4+** - Main testing framework [Line 18]
- **pytest-cov 4.1+** - Code coverage measurement (≥80% target) [Line 19]
- **Python stdlib:** `json`, `csv`, `datetime` modules for export validation

**Coverage Command:**
```bash
poetry run pytest tests/unit/test_audit_repository.py \
  -v \
  --cov=gdpr_pseudonymizer/data/repositories/audit_repository \
  --cov-report=term-missing \
  --cov-report=html
```

**Target Coverage:** ≥90% for AuditRepository (business logic critical for GDPR compliance)

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2026-01-28 | 1.0 | Story created with comprehensive audit logging requirements, export functionality, and GDPR Article 30 mapping | Bob (Scrum Master) |

---

## Dev Agent Record

### Agent Model Used

Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)

### Debug Log References

No debug log entries required. All tasks completed without blocking issues.

### Completion Notes List

- **Task 1-2:** Added `export_to_json()` and `export_to_csv()` methods to [AuditRepository](gdpr_pseudonymizer/data/repositories/audit_repository.py:240-317) with full filter support
- **Task 3:** Created comprehensive test suite with 32 unit tests covering all methods (91.41% coverage)
- **Task 4-5:** Created detailed [audit-logging.md](docs/architecture/audit-logging.md) documentation with:
  - Operations table schema reference
  - Complete AuditRepository usage guide with code examples
  - Common query patterns (filters, date ranges, analytics)
  - Export functionality documentation (JSON + CSV formats)
  - GDPR Article 30 compliance mapping
  - Sample audit report format for compliance officers
  - Integration guidance for Story 2.6
- **Task 6:** Documented integration points in audit-logging.md for Story 2.6 workflow implementation
- **Testing:** All 32 tests pass (100% success rate)
- **Quality Checks (Post-QA):**
  - Ruff linting: ✅ PASS (fixed unused import in test file)
  - Black formatting: ✅ PASS (auto-formatted test_audit_repository.py)
  - Mypy type checking: ✅ PASS (no type errors in 40 source files)
  - Full unit test suite: 261/261 tests PASS, 1 skipped (9.01s runtime)
  - Integration tests: Skipped on Windows (90 tests available, run on Linux/macOS in CI)
- **Coverage:** 91.41% for AuditRepository (exceeds 80% target and 90% business logic target)
- **Standards:** Used absolute imports, type hints on all methods, ISO 8601 datetime formatting, OSError for file I/O errors, timezone-aware datetime handling

### File List

**Modified:**
- [gdpr_pseudonymizer/data/repositories/audit_repository.py](gdpr_pseudonymizer/data/repositories/audit_repository.py) - Added export_to_json() and export_to_csv() methods (lines 18, 240-317); QA refactoring: updated datetime handling to timezone-aware pattern (lines 13, 301)
- [docs/architecture/audit-logging.md](docs/architecture/audit-logging.md) - Complete audit logging documentation with GDPR Article 30 mapping; QA refactoring: corrected error handling documentation from IOError to OSError (lines 297-303)

**Created:**
- [tests/unit/test_audit_repository.py](tests/unit/test_audit_repository.py) - Comprehensive unit tests (32 tests, 1297 lines)

---

## QA Results

### Review Date: 2026-01-28

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Overall Assessment: EXCELLENT**

The audit logging implementation demonstrates exceptional quality across all dimensions:

**Strengths:**
- **Architecture:** Clean repository pattern with proper dependency injection
- **Type Safety:** Complete type hints on all public methods
- **Documentation:** Comprehensive docstrings with usage examples in every method
- **Error Handling:** Proper OSError raising with contextual messages for I/O failures
- **Test Coverage:** 91.41% coverage with 32 comprehensive unit tests
- **Standards Compliance:** All coding standards, project structure, and testing strategy requirements met
- **GDPR Compliance:** Thorough Article 30 mapping with 687-line documentation guide

**Requirements Traceability:**
- AC1 (Operations table): ✓ COMPLETE - Inherited from Story 2.4
- AC2 (FR12 fields): ✓ COMPLETE - All fields logged and tested
- AC3 (User modifications): ✓ COMPLETE - Schema ready for Epic 3
- AC4 (Export functionality): ✓ COMPLETE - JSON/CSV exports with 13 tests
- AC5 (Log rotation): ✓ COMPLETE - Not required for MVP (acknowledged)
- AC6 (Unit tests): ✓ COMPLETE - 32 tests, 91.41% coverage (exceeds 90% target)
- AC7 (Documentation): ✓ COMPLETE - Comprehensive GDPR Article 30 mapping

### Refactoring Performed

**File: [gdpr_pseudonymizer/data/repositories/audit_repository.py](gdpr_pseudonymizer/data/repositories/audit_repository.py)**
- **Change**: Updated `datetime.utcnow()` to `datetime.now(timezone.utc)` on line 301
- **Why**: `datetime.utcnow()` is deprecated in Python 3.12+; using recommended timezone-aware pattern
- **How**: Added `timezone` import and used `datetime.now(timezone.utc)` for export timestamp generation
- **Impact**: Future-proof code for Python 3.12+ while maintaining backward compatibility with 3.9-3.11

**File: [docs/architecture/audit-logging.md](docs/architecture/audit-logging.md)**
- **Change**: Corrected error handling documentation from `IOError` to `OSError` on line 297
- **Why**: Code raises `OSError` but documentation mentioned `IOError` - inconsistency
- **How**: Updated exception type and example code in error handling section
- **Impact**: Documentation now accurately reflects implementation behavior

### Compliance Check

- Coding Standards: ✓ PASS
  - Absolute imports used throughout
  - Type hints on all public methods
  - No sensitive data logged (metadata only)
  - snake_case/PascalCase naming conventions followed

- Project Structure: ✓ PASS
  - Repository in correct location: `gdpr_pseudonymizer/data/repositories/`
  - Tests mirror source structure: `tests/unit/test_audit_repository.py`
  - Documentation in architecture folder: `docs/architecture/audit-logging.md`

- Testing Strategy: ✓ PASS
  - 32 unit tests (75% of test pyramid as required)
  - 91.41% coverage exceeds Epic 2 target (80%) and business logic target (90%)
  - Proper test isolation with tmp_path fixtures
  - All edge cases covered (empty results, non-existent IDs, date ranges, I/O errors)

- All ACs Met: ✓ PASS
  - All 7 acceptance criteria fully implemented and validated

### Improvements Checklist

- [x] Updated datetime handling to timezone-aware pattern (audit_repository.py:13,301)
- [x] Fixed documentation inconsistency for error types (audit-logging.md:297-303)
- [x] Verified all tests pass after refactoring (32/32 tests pass)

**No additional improvements needed** - Implementation is production-ready.

### Security Review

**Status: PASS**

- ✓ No sensitive data logged in operations table (metadata only)
- ✓ Operations table explicitly documented as unencrypted (no PII stored)
- ✓ Sensitive entity data stored separately in encrypted `entities` table
- ✓ Export functionality writes to local files only (NFR11: zero network compliance)
- ✓ Proper error messages without exposing internal details

**GDPR Compliance:**
- ✓ Article 30 (Records of Processing) requirements mapped to operations table fields
- ✓ Article 32 (Security of Processing) demonstrated through audit trail
- ✓ Article 5(2) (Accountability) supported by comprehensive logging
- ✓ Export functionality enables compliance reporting for audits

### Performance Considerations

**Status: PASS**

- ✓ Indexes on `timestamp` and `operation_type` enable fast filtering
- ✓ Query methods return newest-first ordering (optimized for recent operations)
- ✓ Export methods use efficient SQLAlchemy queries (no N+1 problems)
- ✓ JSON/CSV serialization performs well for typical datasets (<10,000 operations)

**Performance Characteristics:**
- Log operation: O(1) database insert
- Query operations: O(log n) with indexes, O(n) result serialization
- Export: O(n) for query + serialization, acceptable for audit logs

**Storage Estimation:** ~500 bytes per operation (documented in audit-logging.md)

### Files Modified During Review

**Modified:**
1. [gdpr_pseudonymizer/data/repositories/audit_repository.py](gdpr_pseudonymizer/data/repositories/audit_repository.py)
   - Line 13: Added `timezone` import
   - Line 301: Updated to `datetime.now(timezone.utc)`

2. [docs/architecture/audit-logging.md](docs/architecture/audit-logging.md)
   - Lines 297-303: Corrected `IOError` → `OSError` in error handling documentation

**Testing:** All 32 tests pass after modifications (verified with `poetry run pytest tests/unit/test_audit_repository.py -v`)

**Note to Dev:** Please update File List section in story to include the refactored files.

### Gate Status

**Gate: PASS** → [docs/qa/gates/2.5-audit-logging.yml](docs/qa/gates/2.5-audit-logging.yml)

**Quality Score:** 100/100

**Summary:** Exceptional implementation with comprehensive test coverage, thorough documentation, and full GDPR Article 30 compliance. All acceptance criteria met. Minor refactoring performed for future-proofing and documentation accuracy. Ready for Story 2.6 integration.

### Recommended Status

**✓ Ready for Done**

Story owner may mark as Done. All acceptance criteria validated, tests passing, documentation complete, and minor improvements applied. The audit logging system is production-ready and prepared for integration in Story 2.6 (Single-Document Workflow).
