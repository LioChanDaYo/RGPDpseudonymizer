name: Performance & Stability Tests

on:
  workflow_dispatch:
    inputs:
      reason:
        description: "Reason for running performance tests"
        required: false
        default: "Manual trigger"
      full_benchmark:
        description: "Run full benchmark (34 rounds per doc size, ~50 min)"
        type: boolean
        default: false
  push:
    branches: ["main"]
    paths:
      - "gdpr_pseudonymizer/core/**"
      - "gdpr_pseudonymizer/nlp/**"
      - "gdpr_pseudonymizer/data/**"
      - "tests/performance/**"

permissions:
  contents: read

env:
  OMP_NUM_THREADS: 1
  PYTHONUTF8: 1

jobs:
  performance:
    runs-on: ubuntu-22.04
    timeout-minutes: 90

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Free disk space
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache Poetry dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pypoetry
            ~/.cache/pip
          key: poetry-deps-ubuntu-3.11-${{ hashFiles('**/poetry.lock') }}

      - name: Install Poetry and dependencies
        run: |
          pip install poetry>=1.7.0
          poetry install --no-interaction

      - name: Cache spaCy model
        uses: actions/cache@v4
        with:
          path: ~/.spacy
          key: spacy-model-fr-core-news-lg-ubuntu

      - name: Download spaCy model
        run: poetry run python -m spacy download fr_core_news_lg

      # Run non-benchmark performance tests (startup, stability, memory, stress)
      - name: Run stability and memory tests
        run: |
          poetry run pytest tests/performance/test_stability.py \
            tests/performance/test_memory_profiling.py \
            tests/performance/test_startup_time.py \
            -v -s --timeout=600 -p no:benchmark \
            --junitxml=performance-stability-results.xml \
            2>&1 | tee performance-stability-output.txt

      # Run stress tests
      - name: Run stress tests
        run: |
          poetry run pytest tests/performance/test_stress.py \
            -v -s --timeout=1200 -p no:benchmark \
            --junitxml=stress-results.xml \
            2>&1 | tee stress-output.txt

      # Run benchmark tests
      # full_benchmark=true: 34 rounds per doc size (~50 min) for authoritative validation
      # full_benchmark=false (default): 3 rounds per doc size (~5 min) for CI regression check
      - name: Run benchmark tests
        run: |
          if [ "${{ github.event.inputs.full_benchmark }}" = "true" ]; then
            BENCH_MIN_ROUNDS="--benchmark-min-rounds=34"
            echo "Running FULL benchmark (34 rounds per doc size)"
          else
            BENCH_MIN_ROUNDS="--benchmark-min-rounds=3"
            echo "Running CI regression check (3 rounds per doc size)"
          fi
          poetry run pytest tests/performance/test_single_document_benchmark.py \
            tests/performance/test_batch_performance.py \
            -v -s --timeout=3600 \
            --benchmark-only --benchmark-autosave \
            ${BENCH_MIN_ROUNDS} \
            --benchmark-json=benchmark-results.json \
            --junitxml=benchmark-results.xml \
            2>&1 | tee benchmark-output.txt

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-results
          path: |
            performance-stability-results.xml
            performance-stability-output.txt
            stress-results.xml
            stress-output.txt
            benchmark-results.xml
            benchmark-results.json
            benchmark-output.txt
            .benchmarks/
          retention-days: 90
