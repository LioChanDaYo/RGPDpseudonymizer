{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dans une biographie autorisée du célèbre écriv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dans un roman épique sur fond de guerre interg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dans un documentaire sur l'art contemporain, J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dans une enquête approfondie sur l'histoire de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dans une biographie autorisée du milliardaire,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Dans une biographie autorisée du célèbre écriv...\n",
       "1  Dans un roman épique sur fond de guerre interg...\n",
       "2  Dans un documentaire sur l'art contemporain, J...\n",
       "3  Dans une enquête approfondie sur l'histoire de...\n",
       "4  Dans une biographie autorisée du milliardaire,..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/dataset_1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a tokenizer function. It takes a string and returns a list of tokens and a list of integers\n",
    "# by default, the integer is 0, but if the token is a first name, the integer is 1; if the token is a last name, the integer is 2\n",
    "# a first name is a token that is followed with the tokens '<', 'first_name', '>'\n",
    "# a last name is a token that is followed with the tokens '<', 'last_name', '>'\n",
    "# in the token list, the tokens '<', 'first_name', '>' and '<', 'last_name', '>' are removed; they have no corresponding integer\n",
    "def tokenizer(s):\n",
    "    tokens = nltk.word_tokenize(s)\n",
    "    ints = [0 for t in tokens]\n",
    "    for i, t in enumerate(tokens):\n",
    "        if i < len(tokens) - 3 and tokens[i + 1] == '<' and tokens[i + 2] == 'first_name' and tokens[i + 3] == '>':\n",
    "            ints[i] = 1\n",
    "            tokens[i + 1] = ''\n",
    "            tokens[i + 2] = ''\n",
    "            tokens[i + 3] = ''\n",
    "        if i < len(tokens) - 3 and tokens[i + 1] == '<' and tokens[i + 2] == 'last_name' and tokens[i + 3] == '>':\n",
    "            ints[i] = 2\n",
    "            tokens[i + 1] = ''\n",
    "            tokens[i + 2] = ''\n",
    "            tokens[i + 3] = ''\n",
    "    # values in ints are kept if the corresponding token is not empty\n",
    "    ints = [i for i, t in zip(ints, tokens) if t != '']\n",
    "    tokens = [t for t in tokens if t != '']\n",
    "    return tokens, ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add two columns to the dataframe: 'tokens' and 'labels'\n",
    "df['tokens'], df['labels'] = zip(*df['text'].map(tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dans une biographie autorisée du célèbre écriv...</td>\n",
       "      <td>[Dans, une, biographie, autorisée, du, célèbre...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 2, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dans un roman épique sur fond de guerre interg...</td>\n",
       "      <td>[Dans, un, roman, épique, sur, fond, de, guerr...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dans un documentaire sur l'art contemporain, J...</td>\n",
       "      <td>[Dans, un, documentaire, sur, l'art, contempor...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dans une enquête approfondie sur l'histoire de...</td>\n",
       "      <td>[Dans, une, enquête, approfondie, sur, l'histo...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dans une biographie autorisée du milliardaire,...</td>\n",
       "      <td>[Dans, une, biographie, autorisée, du, milliar...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 2, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Dans une biographie autorisée du célèbre écriv...   \n",
       "1  Dans un roman épique sur fond de guerre interg...   \n",
       "2  Dans un documentaire sur l'art contemporain, J...   \n",
       "3  Dans une enquête approfondie sur l'histoire de...   \n",
       "4  Dans une biographie autorisée du milliardaire,...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Dans, une, biographie, autorisée, du, célèbre...   \n",
       "1  [Dans, un, roman, épique, sur, fond, de, guerr...   \n",
       "2  [Dans, un, documentaire, sur, l'art, contempor...   \n",
       "3  [Dans, une, enquête, approfondie, sur, l'histo...   \n",
       "4  [Dans, une, biographie, autorisée, du, milliar...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 2, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 2, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lionel\\anaconda3\\envs\\pseudonymizer\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and test sets. The test set is 20% of the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Dan',\n",
       " '##s',\n",
       " 'une',\n",
       " 'bio',\n",
       " '##graph',\n",
       " '##ie',\n",
       " 'auto',\n",
       " '##ris',\n",
       " '##ée',\n",
       " 'du',\n",
       " 'c',\n",
       " '##é',\n",
       " '##l',\n",
       " '##è',\n",
       " '##bre',\n",
       " 'r',\n",
       " '##oman',\n",
       " '##cier',\n",
       " ',',\n",
       " 'Jean',\n",
       " '-',\n",
       " 'Paul',\n",
       " 'Sa',\n",
       " '##rt',\n",
       " '##re',\n",
       " 'ex',\n",
       " '##p',\n",
       " '##lique',\n",
       " 'que',\n",
       " 'Gabriel',\n",
       " 'García',\n",
       " 'M',\n",
       " '##á',\n",
       " '##rque',\n",
       " '##z',\n",
       " 'a',\n",
       " 'personnel',\n",
       " '##lement',\n",
       " 'ins',\n",
       " '##pi',\n",
       " '##ré',\n",
       " 'son',\n",
       " 'style',\n",
       " 'd',\n",
       " \"'\",\n",
       " 'é',\n",
       " '##c',\n",
       " '##rit',\n",
       " '##ure',\n",
       " ',',\n",
       " 'en',\n",
       " 'm',\n",
       " '##ê',\n",
       " '##lant',\n",
       " 'r',\n",
       " '##é',\n",
       " '##alis',\n",
       " '##me',\n",
       " 'ma',\n",
       " '##gi',\n",
       " '##que',\n",
       " 'et',\n",
       " 'exist',\n",
       " '##ential',\n",
       " '##ism',\n",
       " '##e',\n",
       " '.',\n",
       " 'Ce',\n",
       " 'm',\n",
       " '##é',\n",
       " '##lang',\n",
       " '##e',\n",
       " 'de',\n",
       " 'genres',\n",
       " 'a',\n",
       " '##va',\n",
       " '##it',\n",
       " 'é',\n",
       " '##té',\n",
       " 't',\n",
       " '##r',\n",
       " '##ès',\n",
       " 'c',\n",
       " '##rit',\n",
       " '##iq',\n",
       " '##ué',\n",
       " 'par',\n",
       " 'certain',\n",
       " '##s',\n",
       " 'critique',\n",
       " '##s',\n",
       " 'lit',\n",
       " '##té',\n",
       " '##rai',\n",
       " '##res',\n",
       " '.',\n",
       " '`',\n",
       " '`',\n",
       " 'Ma',\n",
       " 'r',\n",
       " '##é',\n",
       " '##po',\n",
       " '##nse',\n",
       " 'à',\n",
       " 'c',\n",
       " '##es',\n",
       " 'critique',\n",
       " '##s',\n",
       " 'est',\n",
       " 'q',\n",
       " '##u',\n",
       " \"'\",\n",
       " 'il',\n",
       " '##s',\n",
       " 'ne',\n",
       " 'com',\n",
       " '##p',\n",
       " '##ren',\n",
       " '##nent',\n",
       " 'to',\n",
       " '##ut',\n",
       " 'simple',\n",
       " '##ment',\n",
       " 'p',\n",
       " '##as',\n",
       " 'mon',\n",
       " 't',\n",
       " '##ra',\n",
       " '##va',\n",
       " '##il',\n",
       " \"'\",\n",
       " \"'\",\n",
       " ',',\n",
       " 'a',\n",
       " '##va',\n",
       " '##it',\n",
       " 'd',\n",
       " '##é',\n",
       " '##c',\n",
       " '##lar',\n",
       " '##é',\n",
       " 'l',\n",
       " \"'\",\n",
       " 'au',\n",
       " '##te',\n",
       " '##ur',\n",
       " 'co',\n",
       " '##lo',\n",
       " '##mb',\n",
       " '##ien',\n",
       " 'lo',\n",
       " '##rs',\n",
       " 'd',\n",
       " \"'\",\n",
       " 'une',\n",
       " 'con',\n",
       " '##f',\n",
       " '##é',\n",
       " '##ren',\n",
       " '##ce',\n",
       " 'à',\n",
       " 'Paris',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the first line of the tokens in the train set ; is_split_into_words=True is needed\n",
    "train_encodings = tokenizer(train_df['tokens'].tolist(), is_split_into_words=True)\n",
    "train_encodings.tokens()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt#processing-the-data\n",
    "# https://huggingface.co/Jean-Baptiste/camembert-ner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pseudonymizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
