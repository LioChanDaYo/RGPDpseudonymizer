{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(columns=[\"first names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get openai api key from .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.8, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remy\n",
      "Séverine\n",
      "Gaspard\n",
      "Jocelyn\n",
      "Séverine\n",
      "Loïc\n",
      "Aurelie\n",
      "Romain\n",
      "Gaspard\n",
      "Gaspard\n",
      "Bastien\n",
      "Aurélien\n",
      "Mael\n",
      "Bastien\n",
      "Léonie\n",
      "Sylvette\n",
      "Thibault\n",
      "Apolline\n",
      "Aubin\n",
      "Célestin\n",
      "Amaury\n",
      "Pascal\n",
      "Gaspard\n",
      "Gaspard\n",
      "Olivier\n",
      "Thibault\n",
      "Thibault\n",
      "Marcelline\n",
      "Jules\n",
      "Celestine\n",
      "Bastien\n",
      "Guillaume\n",
      "Séverin\n",
      "Pascal\n",
      "Remy\n",
      "Eugénie\n",
      "Aurelien\n",
      "Emmeline\n",
      "Jérôme\n",
      "Mael\n",
      "Loïc\n",
      "Pascal\n",
      "Aurelien\n",
      "Gaspard\n",
      "Gabin\n",
      "Aurelien\n",
      "Gaspard\n",
      "Thibault\n",
      "Dorian\n",
      "Sylvette\n",
      "Romain\n",
      "Gaspard\n",
      "Raphaëlle\n",
      "Aurélien\n",
      "Gaspard\n",
      "Guillaume\n",
      "Thibaut\n",
      "Célestin\n",
      "Gael\n",
      "Solène\n",
      "Amélie\n",
      "Gaspard\n",
      "Raphaëlle\n",
      "Rémy\n",
      "Fabien\n",
      "Romain\n",
      "Remy\n",
      "Marceline\n",
      "Adélaïde\n",
      "Loïc\n",
      "Léonide\n",
      "Élodie\n",
      "Léonie\n",
      "Gaspard\n",
      "Aurelien\n",
      "Aurelien\n",
      "Léonide\n",
      "Remy\n",
      "Fabien\n",
      "Théophile\n",
      "Thibaut\n",
      "Aurelien\n",
      "Aurelien\n",
      "Séverine\n",
      "Mathis\n",
      "Emmeline\n",
      "Amélie\n",
      "Maxence\n",
      "Clemence\n",
      "Gervaise\n",
      "Jules\n",
      "Romy\n",
      "Antoine\n",
      "Gabin\n",
      "Théophile\n",
      "Lysandre\n",
      "Jérôme\n",
      "Rémi\n",
      "Remy\n",
      "Raphaëlle\n",
      "Félicien\n",
      "Armand\n",
      "Valentin\n",
      "Léonilde\n",
      "Gwenaëlle\n",
      "Aurelia\n",
      "Romain\n",
      "Armand\n",
      "Élodie\n",
      "Gervaise\n",
      "Remy\n",
      "Célestin\n",
      "Apolline\n",
      "Severin\n",
      "Aurelia\n",
      "Thibault\n",
      "Rémy\n",
      "Célestin\n",
      "Amandine\n",
      "Célestin\n",
      "Célestin\n",
      "Guinevere\n",
      "Romain\n",
      "Bastien\n",
      "Gabin\n",
      "Thierry\n",
      "Théophile\n",
      "Léonie\n",
      "Thibaut\n",
      "Léonide\n",
      "Gervaise\n",
      "Remy\n",
      "Aurelia\n",
      "Bastien\n",
      "Gabin\n",
      "Thibaut\n",
      "Romain\n",
      "Élodie\n",
      "Jérôme\n",
      "Fabien\n",
      "Lazare\n",
      "Gabin\n",
      "Céleste\n",
      "Solène\n",
      "Sylvette\n",
      "Gabin\n",
      "Élodie\n",
      "Loïc\n",
      "Gaspard\n",
      "Marcel\n",
      "Aurelie\n",
      "Remy\n",
      "Gaspard\n",
      "Loïc\n",
      "Gwenaëlle\n",
      "Théophile\n",
      "Raphaël\n",
      "Marceline\n",
      "Gaspard\n",
      "Marcel\n",
      "Yseult\n",
      "Marceline\n",
      "Aurelie\n",
      "Gaspard\n",
      "Sylvette\n",
      "Thibault\n",
      "Théo\n",
      "Thibaud\n",
      "Jules\n",
      "Romain\n",
      "Thibault\n",
      "Remy\n",
      "Raphaëlle\n",
      "Thibault\n",
      "Thibault\n",
      "Géraldine\n",
      "Célestin\n",
      "Théophile\n",
      "Thibaut\n",
      "Raphaëlle\n",
      "Aurelie\n",
      "Séverin\n",
      "Rémi\n",
      "Loïc\n",
      "Sylvette\n",
      "Thibaut\n",
      "Sylvette\n",
      "Aubin\n",
      "Théo\n",
      "Renard\n",
      "Remy\n",
      "Gabin\n",
      "Éloïse\n",
      "Bastien\n",
      "Gabin\n",
      "Thibaut\n",
      "Raphaëlle\n",
      "Théophile\n",
      "Léonie\n",
      "Elodie\n",
      "Sylvette\n",
      "Marcel\n",
      "Baptiste\n",
      "Thibault\n",
      "Jocelyn\n",
      "Armand\n",
      "Célestin\n",
      "Aurelien\n",
      "Gustave\n",
      "Jocelyn\n",
      "Jules\n",
      "Remy\n",
      "Aurelien\n",
      "Gaspard\n",
      "Élodie\n",
      "Guillem\n",
      "Loïc\n",
      "Séverin\n",
      "Loïc\n",
      "Aurelie\n",
      "Thibault\n",
      "Romain\n",
      "Sylvette\n",
      "Gaétan\n",
      "Apolline\n",
      "Gustave\n",
      "Aurelien\n",
      "Séverin\n",
      "Élodie\n",
      "Sylvette\n",
      "Pascal\n",
      "Célestin\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 67df525236c5e1f4d0352c315166c0a5 in your email.) {\n  \"error\": {\n    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 67df525236c5e1f4d0352c315166c0a5 in your email.)\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 67df525236c5e1f4d0352c315166c0a5 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Mon, 18 Sep 2023 20:12:34 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-8qijr3nekkaytfpdjtakwp1m', 'openai-processing-ms': '693', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89935', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '43ms', 'x-request-id': '67df525236c5e1f4d0352c315166c0a5', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '808c3e9409ac2a70-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Lionel\\RGPDpseudonymizer\\nameBaseGenerator.ipynb Cell 4\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lionel/RGPDpseudonymizer/nameBaseGenerator.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(first_name)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lionel/RGPDpseudonymizer/nameBaseGenerator.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwhile\u001b[39;00m first_name \u001b[39min\u001b[39;00m df[\u001b[39m\"\u001b[39m\u001b[39mfirst names\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Lionel/RGPDpseudonymizer/nameBaseGenerator.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     first_name \u001b[39m=\u001b[39m get_completion(prompt)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lionel/RGPDpseudonymizer/nameBaseGenerator.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     first_name \u001b[39m=\u001b[39m first_name\u001b[39m.\u001b[39mstrip(\u001b[39m'\u001b[39m\u001b[39m.,;:?!\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lionel/RGPDpseudonymizer/nameBaseGenerator.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39m(first_name)\n",
      "\u001b[1;32mc:\\Users\\Lionel\\RGPDpseudonymizer\\nameBaseGenerator.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lionel/RGPDpseudonymizer/nameBaseGenerator.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_completion\u001b[39m(prompt, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lionel/RGPDpseudonymizer/nameBaseGenerator.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     messages \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: prompt}]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Lionel/RGPDpseudonymizer/nameBaseGenerator.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lionel/RGPDpseudonymizer/nameBaseGenerator.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lionel/RGPDpseudonymizer/nameBaseGenerator.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lionel/RGPDpseudonymizer/nameBaseGenerator.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.8\u001b[39;49m, \u001b[39m# this is the degree of randomness of the model's output\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lionel/RGPDpseudonymizer/nameBaseGenerator.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lionel/RGPDpseudonymizer/nameBaseGenerator.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Lionel\\anaconda3\\envs\\pseudonymizer\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\Lionel\\anaconda3\\envs\\pseudonymizer\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\Lionel\\anaconda3\\envs\\pseudonymizer\\lib\\site-packages\\openai\\api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    218\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[1;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\Lionel\\anaconda3\\envs\\pseudonymizer\\lib\\site-packages\\openai\\api_requestor.py:620\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    613\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    614\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    615\u001b[0m         )\n\u001b[0;32m    616\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    617\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    619\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 620\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    623\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    624\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    625\u001b[0m         ),\n\u001b[0;32m    626\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    627\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Lionel\\anaconda3\\envs\\pseudonymizer\\lib\\site-packages\\openai\\api_requestor.py:683\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    681\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    682\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 683\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    684\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mAPIError\u001b[0m: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 67df525236c5e1f4d0352c315166c0a5 in your email.) {\n  \"error\": {\n    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 67df525236c5e1f4d0352c315166c0a5 in your email.)\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 67df525236c5e1f4d0352c315166c0a5 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Mon, 18 Sep 2023 20:12:34 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-8qijr3nekkaytfpdjtakwp1m', 'openai-processing-ms': '693', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89935', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '43ms', 'x-request-id': '67df525236c5e1f4d0352c315166c0a5', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '808c3e9409ac2a70-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}"
     ]
    }
   ],
   "source": [
    "# add 10 first names to the dataframe: get them from gpt and, if it is already in the dataframe, get another one\n",
    "prompt = \"I need one and only one random French first name for pseudonymisation purposes. Please do not use the most common French names. Try to find lesser used names. Answer with the first name only.\"\n",
    "for i in range(100):\n",
    "    first_name = get_completion(prompt)\n",
    "    first_name = first_name.strip('.,;:?!\\'\"')\n",
    "    print(first_name)\n",
    "    while first_name in df[\"first names\"].values:\n",
    "        first_name = get_completion(prompt)\n",
    "        first_name = first_name.strip('.,;:?!\\'\"')\n",
    "        print(first_name)\n",
    "    df = pd.concat([df, pd.DataFrame([first_name], columns=[\"first names\"])], ignore_index=True)\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/first_names.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"last names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roussel\n",
      "Lefebvre\n",
      "Lefèvre\n",
      "Girard\n",
      "Dupont\n",
      "Perignon\n",
      "Lefèvre\n",
      "Gagnon\n",
      "Rousselle\n",
      "Girard\n",
      "Bertrand\n",
      "Girard\n",
      "Dupont\n",
      "Duval\n",
      "Lefebvre\n",
      "Durand\n",
      "Baudin\n",
      "Lefèvre\n",
      "Roussel\n",
      "Girard\n",
      "Gagnon\n",
      "Renaud\n",
      "Laroche\n",
      "Rousseau\n",
      "Lefèvre\n",
      "Delacroix\n",
      "Girard\n",
      "Duval\n",
      "Bouchard\n",
      "Lefebvre\n",
      "Mercier\n",
      "Girard\n",
      "Roux\n",
      "Lemoine\n",
      "Bonnell\n",
      "Coulomb\n",
      "Crespel\n",
      "Rousselle\n",
      "Chaput\n",
      "Gagné\n",
      "Bouchard\n",
      "Deschamps\n",
      "Cormier\n",
      "Dubois\n",
      "Gagnon\n",
      "Lefebvre\n",
      "Girard\n",
      "Dumont\n",
      "Dupont\n",
      "Dubois\n",
      "Girard\n",
      "Dumoulin\n",
      "Dubois\n",
      "Lefèvre\n",
      "Dubreuil\n",
      "Lefèvre\n",
      "Dubreuil\n",
      "Briand\n",
      "Dupont\n",
      "Durand\n",
      "Martin\n",
      "Bernard\n",
      "Thomas\n",
      "Lefebvre\n",
      "Robert\n",
      "Richard\n",
      "Petit\n",
      "Moreau\n",
      "Simon\n",
      "Michel\n",
      "Garcia\n",
      "David\n",
      "Bertrand\n",
      "Roux\n",
      "Vincent\n",
      "Fournier\n",
      "Morel\n",
      "Lambert\n",
      "Gérard\n",
      "Dumont\n",
      "Dubois\n",
      "Fontaine\n",
      "Dumont\n",
      "Gagnon\n",
      "Lefort\n",
      "Duval\n",
      "Beauchamp\n",
      "Girard\n",
      "Girard\n",
      "Gagnon\n",
      "D'Amboise\n",
      "Lefèvre\n",
      "Gagnon\n",
      "Leclerc\n",
      "Lefebvre\n",
      "Lefèvre\n",
      "Gervais\n",
      "Lefèvre\n",
      "Lefèvre\n",
      "Lefèvre\n",
      "Gagnon\n",
      "Girard\n",
      "Lefebvre\n",
      "Lefèvre\n",
      "Lemieux\n",
      "Lefebvre\n",
      "Beauchamp\n",
      "Girard\n",
      "Dumont\n",
      "Girard\n",
      "Blanchard\n",
      "Belanger\n",
      "Duval\n",
      "Dumont\n",
      "Gagnon\n",
      "Rousseau\n",
      "Lapierre\n",
      "Rousseau\n",
      "Dubois\n",
      "Lefebvre\n",
      "Lefèvre\n",
      "Durandelle\n",
      "Deschamps\n",
      "Lefèvre\n",
      "Lefevre\n",
      "Girard\n",
      "Girard\n",
      "Duval\n",
      "Gauthier\n",
      "Dupont\n",
      "Fontaine\n",
      "Beauchamp\n",
      "Dubreuil\n",
      "Delacroix\n",
      "Béchard\n",
      "Dupont\n",
      "Dumont\n",
      "Gagnon\n",
      "Dupont\n",
      "Lefevre\n",
      "Girard\n",
      "Girard\n",
      "Bouchard\n",
      "Lefebvre\n",
      "Lefèvre\n",
      "Galliard\n",
      "Duval\n",
      "Dumont\n",
      "Rousseau\n",
      "Dumont\n",
      "Dubois\n",
      "Gagnon\n",
      "Renaud\n",
      "Gagnon\n",
      "Dufour\n",
      "Gervais\n",
      "Durandelle\n",
      "Chevalier\n",
      "Girard\n",
      "Baudin\n",
      "Girard\n",
      "Rousseau\n",
      "Giroux\n",
      "Gagnon\n",
      "Duchamp\n",
      "Lefevre\n",
      "Rousseau\n",
      "Rousseau\n",
      "Dubois\n",
      "Dumont\n",
      "Gagnon\n",
      "Dupont\n",
      "Duverger\n",
      "Boudreau\n",
      "Renaud\n",
      "Bourgeois\n",
      "Dumont\n",
      "Girard\n",
      "Dubois\n",
      "Leclercq\n",
      "Dubois\n",
      "Girard\n",
      "Leblanc\n",
      "Roux\n",
      "Lefèvre\n",
      "Cordier\n",
      "Lefèvre\n",
      "Lefebvre\n",
      "Blanchard\n",
      "Boucher\n",
      "Gautier\n",
      "Gagnon\n",
      "Fontaine\n",
      "Duval\n",
      "Girard\n",
      "1. Blanchet\n",
      "2. Dupuis\n",
      "3. Leclerc\n",
      "4. Mercier\n",
      "5. Renault\n",
      "6. Lefebvre\n",
      "7. Laurent\n",
      "8. Simon\n",
      "9. Moreau\n",
      "10. Da Silva\n",
      "Gautier\n",
      "Leroux\n",
      "Gagnon\n",
      "Delacroix\n",
      "Lefevre\n",
      "Girard\n",
      "Girard\n",
      "Leclerc\n",
      "Lefèvre\n",
      "Cézanne\n",
      "Girard\n",
      "Lefebvre\n",
      "Girard\n",
      "Leclerc\n"
     ]
    }
   ],
   "source": [
    "# add 10 first names to the dataframe: get them from gpt and, if it is already in the dataframe, get another one\n",
    "prompt = \"I need one and only one random French last name for pseudonymisation purposes. Please do not use the most common French names. Try to find lesser used names. Answer with the last name only.\"\n",
    "for i in range(100):\n",
    "    last_name = get_completion(prompt)\n",
    "    last_name = last_name.strip('.,;:?!\\'\"')\n",
    "    print(last_name)\n",
    "    while last_name in df[\"last names\"].values:\n",
    "        last_name = get_completion(prompt)\n",
    "        last_name = last_name.strip('.,;:?!\\'\"')\n",
    "        print(last_name)\n",
    "    df = pd.concat([df, pd.DataFrame([last_name], columns=[\"last names\"])], ignore_index=True)\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Lionel\\RGPDpseudonymizer\\nameBaseGenerator.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Lionel/RGPDpseudonymizer/nameBaseGenerator.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mdata/last_names.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"data/last_names.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RGPDpseudonymizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
